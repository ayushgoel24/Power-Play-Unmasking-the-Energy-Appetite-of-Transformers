{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snip+resnet34_own_layerwise_every_epoch_pruning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI-oLU4e7tt4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "import copy\n",
        "import types"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkEc3pBV7_1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efaa665d-449d-4410-8090-6f682050039d"
      },
      "source": [
        "torch.manual_seed(42)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fcd509147d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdlVLd6c8A9I",
        "outputId": "84bac4be-d9ff-4783-9d83-a46fddd72448"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_KM6Zir8Dbu"
      },
      "source": [
        "def snip_forward_conv2d(self, x):\n",
        "        return F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
        "                        self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def snip_forward_linear(self, x):\n",
        "        return F.linear(x, self.weight * self.weight_mask, self.bias)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5Bqzp2TlzUJ"
      },
      "source": [
        "def SNIP_mask_add(net):\n",
        "    # TODO: shuffle?\n",
        "\n",
        "    #net = copy.deepcopy(net)\n",
        "\n",
        "    #grads_abs = []\n",
        "\n",
        "    for layer in net.modules():\n",
        "        #print(\"This is layer\", layer)\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
        "            #nn.init.xavier_normal_(layer.weight)\n",
        "            #layer.weight.requires_grad = False\n",
        "            #print(\"abcd\")\n",
        "            #print(layer.weight_mask)\n",
        "\n",
        "        # Override the forward methods:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.forward = types.MethodType(snip_forward_conv2d, layer)\n",
        "\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            layer.forward = types.MethodType(snip_forward_linear, layer)\n",
        "\n",
        "def SNIP_mask_quantize(net, keep_ratio):\n",
        "    #grads_abs=[]\n",
        "    layer_num=0\n",
        "    keep_masks = []\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            grads_abs = torch.abs(layer.weight_mask.grad)\n",
        "\n",
        "            # Gather all scores in a single vector and normalise\n",
        "            all_scores = torch.flatten(grads_abs) \n",
        "            norm_factor = torch.sum(all_scores)\n",
        "            all_scores.div_(norm_factor)\n",
        "\n",
        "            num_params_to_keep = int(len(all_scores) * keep_ratio[layer_num])\n",
        "            threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
        "            acceptable_score = threshold[-1]\n",
        "            \n",
        "\n",
        "            #for g in grads_abs:\n",
        "            #g = grad_abs\n",
        "            keep_masks.append(((grads_abs / norm_factor) >= acceptable_score).float())\n",
        "            #print(grads_abs.shape, all_scores.shape, layer_num, num_params_to_keep, acceptable_score, norm_factor)\n",
        "            layer_num +=1\n",
        "            #print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))\n",
        "\n",
        "    return (keep_masks)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggkjQer1sLZn"
      },
      "source": [
        "def apply_prune_mask(net, keep_masks):\n",
        "\n",
        "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
        "    # one-to-one by removing all the irrelevant modules:\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
        "            layer, nn.Linear), net.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "\n",
        "        # mask[i] == 0 --> Prune parameter\n",
        "        # mask[i] == 1 --> Keep parameter\n",
        "\n",
        "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
        "        # Step 2: Make sure their gradients remain zero\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSPbHdYp8K6d"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, base_width=1, padding=1, batch_norm=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if batch_norm is None:\n",
        "            bn_layer = nn.Batchnorm2D\n",
        "        else:\n",
        "            bn_layer = batch_norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=False)\n",
        "        self.bn1 = bn_layer(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn2 = bn_layer(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        if self.downsample is None:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "            self.shortcut = self.downsample\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet34(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, num_classes=10, zero_init_residual=False, base_width=64, batch_norm=None):\n",
        "        # def make_layer(self, block, planes, blocks, stride= 1, dilate = False):\n",
        "        super(ResNet34, self).__init__()\n",
        "        block = BasicBlock\n",
        "        if batch_norm is None:\n",
        "            bn_layer = nn.BatchNorm2d\n",
        "        self.bn_layer = bn_layer\n",
        "\n",
        "        self.conv_out_channels = 64\n",
        "        self.in_channels = self.conv_out_channels\n",
        "        self.base_width = base_width\n",
        "        self.conv1 = nn.Conv2d(3, self.conv_out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = bn_layer(self.conv_out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self.container(block, 64, layers[0])\n",
        "        self.layer2 = self.container(block, 128, layers[1], stride=2, dilate=False)\n",
        "        self.layer3 = self.container(block, 256, layers[2], stride=2, dilate=False)\n",
        "        self.layer4 = self.container(block, 512, layers[3], stride=2, dilate=False)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                #nn.init.xavier_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                nn.init.xavier_normal_(m.weight)\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def container(self, block, in_channels, num_basicblocks, stride=1, dilate=False):\n",
        "        bn_layer = self.bn_layer\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, in_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                bn_layer(in_channels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.in_channels, in_channels, stride, downsample, self.base_width, padding=1, batch_norm=bn_layer))\n",
        "        self.in_channels = in_channels\n",
        "        for basic_blocks in range(1, num_basicblocks):\n",
        "            layers.append(block(self.in_channels, in_channels, base_width=self.base_width, batch_norm=bn_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1twTvZ5RI8A"
      },
      "source": [
        "def network_init():\n",
        "    \n",
        "    \n",
        "  net = ResNet34([3, 4, 6, 3])\n",
        "  optimiser = optim.SGD( net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimiser, lr_decay_interval, gamma=0.1)\n",
        "\n",
        "\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  train_dataset = CIFAR10('_dataset', True, train_transform, download=True)\n",
        "  test_dataset = CIFAR10('_dataset', False, test_transform, download=False)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "  val_loader = DataLoader( test_dataset, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "  return net, optimiser, scheduler, train_loader, val_loader"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxL3nlg-RMoL"
      },
      "source": [
        "def training(epoch, model, optimizer, scheduler, criterion, device, train_loader):\n",
        "  model.train()\n",
        "  avg_loss = 0.0\n",
        "  av_loss=0.0\n",
        "  total=0\n",
        "  for batch_num, (feats, labels) in enumerate(train_loader):\n",
        "      feats, labels = feats.to(device), labels.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(feats)\n",
        "\n",
        "\n",
        "      loss = criterion(outputs, labels.long())\n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      \n",
        "      avg_loss += loss.item()\n",
        "      av_loss += loss.item() \n",
        "      total +=len(feats) \n",
        "      # if batch_num % 10 == 9:\n",
        "      #     print('Epoch: {}\\tBatch: {}\\tAv-Loss: {:.4f}'.format(epoch+1, batch_num+1, av_loss/10))\n",
        "      #     av_loss = 0.0\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      del feats\n",
        "      del labels\n",
        "      del loss\n",
        "\n",
        "  del train_loader\n",
        "\n",
        "  return avg_loss/total\n",
        "  "
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hY8fgwrIRQEm"
      },
      "source": [
        "def validate(epoch, model, criterion, device, data_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss, accuracy,total  = 0.0, 0.0, 0\n",
        "\n",
        "        \n",
        "        for i, (X, Y) in enumerate(data_loader):\n",
        "            \n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            output= model(X)\n",
        "            loss = criterion(output, Y.long())\n",
        "\n",
        "            _,pred_labels = torch.max(F.softmax(output, dim=1), 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            \n",
        "            accuracy += torch.sum(torch.eq(pred_labels, Y)).item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total += len(X)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            del X\n",
        "            del Y\n",
        "        \n",
        "        return running_loss/total, accuracy/total"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKXEzDTJD9WA"
      },
      "source": [
        "batch_size = 128\n",
        "lr = 0.1\n",
        "weight_decay = 0.0005\n",
        "epochs = 70\n",
        "lr_decay_interval = 20"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKuggE1YexRs"
      },
      "source": [
        "# lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "#         optimiser, 20, gamma=0.1)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNnI26UhRUbZ"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "      net, optimiser, lr_scheduler, train_loader, val_loader = network_init()\n",
        "      net = net.to(device)\n",
        "      # Pre-training pruning using SKIP\n",
        "      #keep_masks = SNIP(net, 0.01, train_loader, device)  # TODO: shuffle?\n",
        "      #apply_prune_mask(net, keep_masks)\n",
        "      keep_ratio = [1] * 37\n",
        "      keep_ratio[5:36] = [0.005]*32\n",
        "      keep_ratio[:4]=[0.05]*5\n",
        "      \n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          #print(\"This is model\", sum(1 for _ in net.parameters()))\n",
        "          SNIP_mask_add(net)\n",
        "\n",
        "          train_loss = training(epoch, net, optimiser, lr_scheduler, criterion, device,train_loader)\n",
        "\n",
        "          val_loss, val_acc = validate(epoch, net, criterion, device, val_loader)\n",
        "\n",
        "          lr_scheduler.step()\n",
        "\n",
        "          keep_masks = SNIP_mask_quantize(net, keep_ratio)  # TODO: shuffle?\n",
        "          apply_prune_mask(net, keep_masks)\n",
        "\n",
        "          #print(net.layer[0], net.layer[0].weight.shape)\n",
        "          print('Epoch: {} \\t train-Loss: {:.4f}, \\tval-Loss: {:.4f}, \\tval-acc: {:.4f}'.format(epoch+1,  train_loss, val_loss, val_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FexAGq1Ezuv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
