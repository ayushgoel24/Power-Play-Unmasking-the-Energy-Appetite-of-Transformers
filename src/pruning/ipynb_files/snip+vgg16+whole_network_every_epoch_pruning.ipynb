{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "snip+vgg16+whole_network_every_epoch_pruning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1IxBW0XLDrP"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "\n",
        "import copy\n",
        "import types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAZkQu8lLOz0"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHCESUJ9LSLd"
      },
      "source": [
        "def snip_forward_conv2d(self, x):\n",
        "        return F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
        "                        self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def snip_forward_linear(self, x):\n",
        "        return F.linear(x, self.weight * self.weight_mask, self.bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECjQ0rsZLT0u"
      },
      "source": [
        "def SNIP_mask_add(net):\n",
        "    # TODO: shuffle?\n",
        "\n",
        "    # removed network deep copy and added mask parameters directly into our own network\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
        "            #nn.init.xavier_normal_(layer.weight)\n",
        "            #layer.weight.requires_grad = False\n",
        "            #print(\"abcd\")\n",
        "            #print(layer.weight_mask)\n",
        "\n",
        "        # Override the forward methods:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.forward = types.MethodType(snip_forward_conv2d, layer)\n",
        "\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            layer.forward = types.MethodType(snip_forward_linear, layer)\n",
        "\n",
        "\n",
        "def SNIP_mask_quantize(net, keep_ratio):\n",
        "    # finding the top keep_ratio percentage of weights through out the network.\n",
        "\n",
        "    grads_abs=[]\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            grads_abs.append(torch.abs(layer.weight_mask.grad))\n",
        "\n",
        "    # Gather all scores in a single vector and normalise\n",
        "    all_scores = torch.cat([torch.flatten(x) for x in grads_abs])\n",
        "    norm_factor = torch.sum(all_scores)\n",
        "    all_scores.div_(norm_factor)\n",
        "\n",
        "    num_params_to_keep = int(len(all_scores) * keep_ratio)\n",
        "    threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
        "    acceptable_score = threshold[-1]\n",
        "\n",
        "    keep_masks = []\n",
        "    for g in grads_abs:\n",
        "        keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
        "        \n",
        "    \n",
        "    return (keep_masks)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSTBkGaRLU0g"
      },
      "source": [
        "def apply_prune_mask(net, keep_masks):\n",
        "\n",
        "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
        "    # one-to-one by removing all the irrelevant modules:\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
        "            layer, nn.Linear), net.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "        \n",
        "        # mask[i] == 0 --> Prune parameter\n",
        "        # mask[i] == 1 --> Keep parameter\n",
        "\n",
        "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
        "        # Step 2: Make sure their gradients remain zero\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ueM6tedLzsK"
      },
      "source": [
        "class VGG16(nn.Module):\n",
        "   def __init__(self, config, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = self.make_layers(config, batch_norm=True)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # 512 * 7 * 7 in the original VGG\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),  # instead of dropout\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),  # instead of dropout\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def make_layers(config, batch_norm=False):  # TODO: BN yes or no?\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for v in config:\n",
        "            if v == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "                if batch_norm:\n",
        "                    layers += [\n",
        "                        conv2d,\n",
        "                        nn.BatchNorm2d(v),\n",
        "                        nn.ReLU(inplace=True)\n",
        "                    ]\n",
        "                else:\n",
        "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "                in_channels = v\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)  \n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnwIvwMyLUyN"
      },
      "source": [
        "def network_init():\n",
        "    \n",
        "    \n",
        "  net = VGG16([ 64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M',\n",
        "        512, 512, 512, 'M'\n",
        "    ])\n",
        "  optimiser = optim.SGD( net.parameters(), lr=lr, momentum=0.9, weight_decay=weight_decay)\n",
        "  scheduler = optim.lr_scheduler.StepLR(optimiser, lr_decay_interval, gamma=0.1)\n",
        "\n",
        "\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.RandomCrop(32, padding=4),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                          (0.2023, 0.1994, 0.2010)),\n",
        "  ])\n",
        "\n",
        "  train_dataset = CIFAR10('_dataset', True, train_transform, download=True)\n",
        "  test_dataset = CIFAR10('_dataset', False, test_transform, download=False)\n",
        "\n",
        "  train_loader = DataLoader( train_dataset, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "  val_loader = DataLoader( test_dataset, batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "  return net, optimiser, scheduler, train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKpvqbUyLUvT"
      },
      "source": [
        "def training(epoch, model, optimizer, scheduler, criterion, device, train_loader):\n",
        "  model.train()\n",
        "  avg_loss = 0.0\n",
        "  av_loss=0.0\n",
        "  total=0\n",
        "  for batch_num, (feats, labels) in enumerate(train_loader):\n",
        "      feats, labels = feats.to(device), labels.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(feats)\n",
        "\n",
        "\n",
        "      loss = criterion(outputs, labels.long())\n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      \n",
        "      avg_loss += loss.item()\n",
        "      av_loss += loss.item() \n",
        "      total +=len(feats) \n",
        "      # if batch_num % 10 == 9:\n",
        "      #     print('Epoch: {}\\tBatch: {}\\tAv-Loss: {:.4f}'.format(epoch+1, batch_num+1, av_loss/10))\n",
        "      #     av_loss = 0.0\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      del feats\n",
        "      del labels\n",
        "      del loss\n",
        "\n",
        "  del train_loader\n",
        "\n",
        "  return avg_loss/total\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTNAyKMRLUsh"
      },
      "source": [
        "def validate(epoch, model, criterion, device, data_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss, accuracy,total  = 0.0, 0.0, 0\n",
        "\n",
        "        \n",
        "        for i, (X, Y) in enumerate(data_loader):\n",
        "            \n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            output= model(X)\n",
        "            loss = criterion(output, Y.long())\n",
        "\n",
        "            _,pred_labels = torch.max(F.softmax(output, dim=1), 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            \n",
        "            accuracy += torch.sum(torch.eq(pred_labels, Y)).item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total += len(X)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            del X\n",
        "            del Y\n",
        "        \n",
        "        return running_loss/total, accuracy/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gahI_lMsLUon"
      },
      "source": [
        "batch_size = 128\n",
        "lr = 0.1\n",
        "weight_decay = 0.0005\n",
        "epochs = 70\n",
        "lr_decay_interval = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4uyvwh4LUfR"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "      net, optimiser, lr_scheduler, train_loader, val_loader = network_init()\n",
        "      net = net.to(device)\n",
        "      keep_ratio=0.05\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "      for epoch in range(epochs):\n",
        "          SNIP_mask_add(net)\n",
        "\n",
        "          train_loss = training(epoch, net, optimiser, lr_scheduler, criterion, device,train_loader)\n",
        "\n",
        "          val_loss, val_acc = validate(epoch, net, criterion, device, val_loader)\n",
        "\n",
        "          lr_scheduler.step()\n",
        "\n",
        "          keep_masks = SNIP_mask_quantize(net, keep_ratio)  # TODO: shuffle?\n",
        "          apply_prune_mask(net, keep_masks)\n",
        "\n",
        "          \n",
        "          print('Epoch: {} \\t train-Loss: {:.4f}, \\tval-Loss: {:.4f}, \\tval-acc: {:.4f}'.format(epoch+1,  train_loss, val_loss, val_acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}