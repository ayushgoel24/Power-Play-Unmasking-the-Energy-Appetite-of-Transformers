{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Snip+Vgg16.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV2npeoKvTHO",
        "outputId": "d6659eea-e179-4663-b293-9d2ef3712351"
      },
      "source": [
        "!pip install tensorboardX\n",
        "!pip install pytorch_ignite"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/84/46421bd3e0e89a92682b1a38b40efc22dafb6d8e3d947e4ceefd4a5fabc7/tensorboardX-2.2-py2.py3-none-any.whl (120kB)\n",
            "\r\u001b[K     |██▊                             | 10kB 22.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 30kB 14.8MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 40kB 13.6MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 51kB 8.9MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 61kB 9.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 81kB 10.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 92kB 10.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 8.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (54.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.2\n",
            "Collecting pytorch_ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f8/d3/640f70d69393b415e6a29b27c735047ad86267921ad62682d1d756556d48/pytorch_ignite-0.4.4-py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 9.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.7/dist-packages (from pytorch_ignite) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch_ignite) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch<2,>=1.3->pytorch_ignite) (1.19.5)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6b4hMWLui51"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST, CIFAR10\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "\n",
        "#from tensorboardX import SummaryWriter\n",
        "#from ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\n",
        "#from ignite.metrics import Accuracy, Loss\n",
        "#from ignite.contrib.handlers import ProgressBar\n",
        "\n",
        "#from snip import SNIP\n",
        "import copy\n",
        "import types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yzYQ1qyuolI"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "LOG_INTERVAL = 20\n",
        "INIT_LR = 0.1\n",
        "WEIGHT_DECAY_RATE = 0.0005\n",
        "EPOCHS = 70\n",
        "REPEAT_WITH_DIFFERENT_SEED = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqCo7rzNus6t"
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CFBia8VwCW1",
        "outputId": "0a1d01eb-cfc2-4f28-8cce-8ad394b9cac1"
      },
      "source": [
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ASAwtASv1Uc"
      },
      "source": [
        "def snip_forward_conv2d(self, x):\n",
        "        return F.conv2d(x, self.weight * self.weight_mask, self.bias,\n",
        "                        self.stride, self.padding, self.dilation, self.groups)\n",
        "\n",
        "\n",
        "def snip_forward_linear(self, x):\n",
        "        return F.linear(x, self.weight * self.weight_mask, self.bias)\n",
        "\n",
        "\n",
        "def SNIP(net, keep_ratio, train_dataloader, device):\n",
        "    # TODO: shuffle?\n",
        "\n",
        "    # Grab a single batch from the training dataset\n",
        "    inputs, targets = next(iter(train_dataloader))\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "\n",
        "    # Let's create a fresh copy of the network so that we're not worried about\n",
        "    # affecting the actual training-phase\n",
        "    net = copy.deepcopy(net)\n",
        "\n",
        "    # Monkey-patch the Linear and Conv2d layer to learn the multiplicative mask\n",
        "    # instead of the weights\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            layer.weight_mask = nn.Parameter(torch.ones_like(layer.weight))\n",
        "            nn.init.xavier_normal_(layer.weight)\n",
        "            layer.weight.requires_grad = False\n",
        "\n",
        "        # Override the forward methods:\n",
        "        if isinstance(layer, nn.Conv2d):\n",
        "            layer.forward = types.MethodType(snip_forward_conv2d, layer)\n",
        "\n",
        "        if isinstance(layer, nn.Linear):\n",
        "            layer.forward = types.MethodType(snip_forward_linear, layer)\n",
        "\n",
        "    # Compute gradients (but don't apply them)\n",
        "    net.zero_grad()\n",
        "    outputs = net.forward(inputs)\n",
        "    loss = F.nll_loss(outputs, targets)\n",
        "    loss.backward()\n",
        "\n",
        "    grads_abs = []\n",
        "    for layer in net.modules():\n",
        "        if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
        "            grads_abs.append(torch.abs(layer.weight_mask.grad))\n",
        "\n",
        "    # Gather all scores in a single vector and normalise\n",
        "    all_scores = torch.cat([torch.flatten(x) for x in grads_abs])\n",
        "    norm_factor = torch.sum(all_scores)\n",
        "    all_scores.div_(norm_factor)\n",
        "\n",
        "    num_params_to_keep = int(len(all_scores) * keep_ratio)\n",
        "    threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
        "    acceptable_score = threshold[-1]\n",
        "\n",
        "    keep_masks = []\n",
        "    for g in grads_abs:\n",
        "        keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
        "        \n",
        "    print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))\n",
        "\n",
        "    return (keep_masks)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lPVEVhUuvk9"
      },
      "source": [
        "def apply_prune_mask(net, keep_masks):\n",
        "\n",
        "    # Before I can zip() layers and pruning masks I need to make sure they match\n",
        "    # one-to-one by removing all the irrelevant modules:\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
        "            layer, nn.Linear), net.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "\n",
        "        def hook_factory(keep_mask):\n",
        "            \"\"\"\n",
        "            The hook function can't be defined directly here because of Python's\n",
        "            late binding which would result in all hooks getting the very last\n",
        "            mask! Getting it through another function forces early binding.\n",
        "            \"\"\"\n",
        "\n",
        "            def hook(grads):\n",
        "                return grads * keep_mask\n",
        "\n",
        "            return hook\n",
        "\n",
        "        # mask[i] == 0 --> Prune parameter\n",
        "        # mask[i] == 1 --> Keep parameter\n",
        "\n",
        "        # Step 1: Set the masked weights to zero (NB the biases are ignored)\n",
        "        # Step 2: Make sure their gradients remain zero\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n",
        "        layer.weight.register_hook(hook_factory(keep_mask))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMrWi0uru0KI"
      },
      "source": [
        "VGG_CONFIGS = {\n",
        "    # M for MaxPool, Number for channels\n",
        "    'D': [\n",
        "        64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M',\n",
        "        512, 512, 512, 'M'\n",
        "    ],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG_SNIP(nn.Module):\n",
        "    \"\"\"\n",
        "    This is a base class to generate three VGG variants used in SNIP paper:\n",
        "        1. VGG-C (16 layers)\n",
        "        2. VGG-D (16 layers)\n",
        "        3. VGG-like\n",
        "\n",
        "    Some of the differences:\n",
        "        * Reduced size of FC layers to 512\n",
        "        * Adjusted flattening to match CIFAR-10 shapes\n",
        "        * Replaced dropout layers with BatchNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = self.make_layers(VGG_CONFIGS[config], batch_norm=True)\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512, 512),  # 512 * 7 * 7 in the original VGG\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),  # instead of dropout\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(True),\n",
        "            nn.BatchNorm1d(512),  # instead of dropout\n",
        "            nn.Linear(512, num_classes),\n",
        "        )\n",
        "\n",
        "    @staticmethod\n",
        "    def make_layers(config, batch_norm=False):  # TODO: BN yes or no?\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for v in config:\n",
        "            if v == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n",
        "                if batch_norm:\n",
        "                    layers += [\n",
        "                        conv2d,\n",
        "                        nn.BatchNorm2d(v),\n",
        "                        nn.ReLU(inplace=True)\n",
        "                    ]\n",
        "                else:\n",
        "                    layers += [conv2d, nn.ReLU(inplace=True)]\n",
        "                in_channels = v\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)  \n",
        "        x = F.log_softmax(x, dim=1)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMCMk60Ju-gE"
      },
      "source": [
        "def get_cifar10_dataloaders(train_batch_size, test_batch_size):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = CIFAR10('_dataset', True, train_transform, download=True)\n",
        "    test_dataset = CIFAR10('_dataset', False, test_transform, download=False)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        test_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wvk6YN8xu_ME"
      },
      "source": [
        "def cifar10_experiment():\n",
        "    \n",
        "    BATCH_SIZE = 128\n",
        "    LR_DECAY_INTERVAL = 20\n",
        "    \n",
        "    #net = VGG_SNIP('D').to(device)\n",
        "    net = \n",
        "    optimiser = optim.SGD(\n",
        "        net.parameters(),\n",
        "        lr=INIT_LR,\n",
        "        momentum=0.9,\n",
        "        weight_decay=WEIGHT_DECAY_RATE)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimiser, LR_DECAY_INTERVAL, gamma=0.1)\n",
        "    \n",
        "    train_loader, val_loader = get_cifar10_dataloaders(BATCH_SIZE,\n",
        "                                                       BATCH_SIZE)  # TODO\n",
        "\n",
        "    return net, optimiser, lr_scheduler, train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPf_jTvPThqO"
      },
      "source": [
        "def training(epoch, model, optimizer, scheduler, criterion, device, train_loader):\n",
        "  model.train()\n",
        "  avg_loss = 0.0\n",
        "  av_loss=0.0\n",
        "  total=0\n",
        "  for batch_num, (feats, labels) in enumerate(train_loader):\n",
        "      feats, labels = feats.to(device), labels.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(feats)\n",
        "\n",
        "\n",
        "      loss = criterion(outputs, labels.long())\n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      \n",
        "      avg_loss += loss.item()\n",
        "      av_loss += loss.item() \n",
        "      total +=len(feats) \n",
        "      # if batch_num % 10 == 9:\n",
        "      #     print('Epoch: {}\\tBatch: {}\\tAv-Loss: {:.4f}'.format(epoch+1, batch_num+1, av_loss/10))\n",
        "      #     av_loss = 0.0\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      del feats\n",
        "      del labels\n",
        "      del loss\n",
        "\n",
        "  del train_loader\n",
        "\n",
        "  return avg_loss/total\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozre5-DeirOd"
      },
      "source": [
        "def validate(epoch, model, criterion, device, data_loader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss, accuracy,total  = 0.0, 0.0, 0\n",
        "\n",
        "        \n",
        "        for i, (X, Y) in enumerate(data_loader):\n",
        "            \n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            output= model(X)\n",
        "            loss = criterion(output, Y.long())\n",
        "\n",
        "            _,pred_labels = torch.max(F.softmax(output, dim=1), 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            \n",
        "            accuracy += torch.sum(torch.eq(pred_labels, Y)).item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total += len(X)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            del X\n",
        "            del Y\n",
        "        \n",
        "        return running_loss/total, accuracy/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vpA1wxDvLKG"
      },
      "source": [
        "def train():\n",
        "\n",
        "    writer = SummaryWriter()\n",
        "\n",
        "    net, optimiser, lr_scheduler, train_loader, val_loader = cifar10_experiment()\n",
        "\n",
        "    # Pre-training pruning using SKIP\n",
        "    keep_masks = SNIP(net, 0.05, train_loader, device)  # TODO: shuffle?\n",
        "    apply_prune_mask(net, keep_masks)\n",
        "    \n",
        "    \"\"\"trainer = create_supervised_trainer(net, optimiser, F.nll_loss, device)\n",
        "    evaluator = create_supervised_evaluator(net, {\n",
        "        'accuracy': Accuracy(),\n",
        "        'nll': Loss(F.nll_loss)\n",
        "    }, device)\n",
        "\n",
        "    pbar = ProgressBar()\n",
        "    pbar.attach(trainer)\n",
        "\n",
        "    @trainer.on(Events.ITERATION_COMPLETED)\n",
        "    def log_training_loss(engine):\n",
        "        lr_scheduler.step()\n",
        "        iter_in_epoch = (engine.state.iteration - 1) % len(train_loader) + 1\n",
        "        if engine.state.iteration % LOG_INTERVAL == 0:\n",
        "            # pbar.log_message(\"Epoch[{}] Iteration[{}/{}] Loss: {:.2f}\"\n",
        "            #       \"\".format(engine.state.epoch, iter_in_epoch, len(train_loader), engine.state.output))\n",
        "            # writer.add_scalar(\"training/loss\", engine.state.output,\n",
        "            #                   engine.state.iteration)\n",
        "            print(\"training/loss\", engine.state.output,\n",
        "                               engine.state.iteration)\n",
        "\n",
        "    @trainer.on(Events.EPOCH_COMPLETED)\n",
        "    def log_epoch(engine):\n",
        "        evaluator.run(val_loader)\n",
        "\n",
        "        metrics = evaluator.state.metrics\n",
        "        avg_accuracy = metrics['accuracy']\n",
        "        avg_nll = metrics['nll']\n",
        "\n",
        "        # pbar.log_message(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n",
        "        #       .format(engine.state.epoch, avg_accuracy, avg_nll))\n",
        "\n",
        "        writer.add_scalar(\"validation/loss\", avg_nll, engine.state.iteration)\n",
        "        writer.add_scalar(\"validation/accuracy\", avg_accuracy,\n",
        "                          engine.state.iteration)\n",
        "\n",
        "    trainer.run(train_loader, EPOCHS)\n",
        "\n",
        "    # Let's look at the final weights\n",
        "    # for name, param in net.named_parameters():\n",
        "    #     if name.endswith('weight'):\n",
        "    #         writer.add_histogram(name, param)\n",
        "\n",
        "    writer.close()\"\"\"\n",
        "    #training(net, optimiser, scheduler,F.nll_loss, device)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAheJvDdwp68",
        "outputId": "9ee224df-5290-4fd2-8ddc-36c6dc759035"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    for _ in range(REPEAT_WITH_DIFFERENT_SEED):\n",
        "      #train()\n",
        "      net, optimiser, lr_scheduler, train_loader, val_loader = cifar10_experiment()\n",
        "      net = net.to(device)\n",
        "      # Pre-training pruning using SKIP\n",
        "      keep_masks = SNIP(net, 0.05, train_loader, device)  # TODO: shuffle?\n",
        "      apply_prune_mask(net, keep_masks)\n",
        "      \n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      for epoch in range(EPOCHS):\n",
        "          train_loss = training(epoch, net, optimiser, lr_scheduler, criterion, device,train_loader)\n",
        "\n",
        "          val_loss, val_acc = validate(epoch, net, criterion, device, val_loader)\n",
        "\n",
        "          lr_scheduler.step()\n",
        "\n",
        "          print('Epoch: {} \\t train-Loss: {:.4f}, \\tval-Loss: {:.4f}, \\tval-acc: {:.4f}'.format(epoch+1,  train_loss, val_loss, val_acc))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "tensor(761993, device='cuda:0')\n",
            "Epoch: 1 \t train-Loss: 0.0120, \tval-Loss: 0.0094, \tval-acc: 0.5663\n",
            "Epoch: 2 \t train-Loss: 0.0078, \tval-Loss: 0.0070, \tval-acc: 0.6875\n",
            "Epoch: 3 \t train-Loss: 0.0062, \tval-Loss: 0.0060, \tval-acc: 0.7340\n",
            "Epoch: 4 \t train-Loss: 0.0053, \tval-Loss: 0.0061, \tval-acc: 0.7342\n",
            "Epoch: 5 \t train-Loss: 0.0049, \tval-Loss: 0.0066, \tval-acc: 0.7186\n",
            "Epoch: 6 \t train-Loss: 0.0046, \tval-Loss: 0.0055, \tval-acc: 0.7583\n",
            "Epoch: 7 \t train-Loss: 0.0044, \tval-Loss: 0.0062, \tval-acc: 0.7342\n",
            "Epoch: 8 \t train-Loss: 0.0042, \tval-Loss: 0.0065, \tval-acc: 0.7254\n",
            "Epoch: 9 \t train-Loss: 0.0041, \tval-Loss: 0.0044, \tval-acc: 0.8062\n",
            "Epoch: 10 \t train-Loss: 0.0040, \tval-Loss: 0.0048, \tval-acc: 0.7946\n",
            "Epoch: 11 \t train-Loss: 0.0040, \tval-Loss: 0.0054, \tval-acc: 0.7779\n",
            "Epoch: 12 \t train-Loss: 0.0039, \tval-Loss: 0.0049, \tval-acc: 0.7933\n",
            "Epoch: 13 \t train-Loss: 0.0038, \tval-Loss: 0.0055, \tval-acc: 0.7692\n",
            "Epoch: 14 \t train-Loss: 0.0037, \tval-Loss: 0.0053, \tval-acc: 0.7770\n",
            "Epoch: 15 \t train-Loss: 0.0037, \tval-Loss: 0.0054, \tval-acc: 0.7638\n",
            "Epoch: 16 \t train-Loss: 0.0036, \tval-Loss: 0.0055, \tval-acc: 0.7631\n",
            "Epoch: 17 \t train-Loss: 0.0036, \tval-Loss: 0.0048, \tval-acc: 0.7946\n",
            "Epoch: 18 \t train-Loss: 0.0035, \tval-Loss: 0.0073, \tval-acc: 0.7237\n",
            "Epoch: 19 \t train-Loss: 0.0035, \tval-Loss: 0.0049, \tval-acc: 0.7835\n",
            "Epoch: 20 \t train-Loss: 0.0035, \tval-Loss: 0.0048, \tval-acc: 0.7915\n",
            "Epoch: 21 \t train-Loss: 0.0023, \tval-Loss: 0.0024, \tval-acc: 0.8957\n",
            "Epoch: 22 \t train-Loss: 0.0019, \tval-Loss: 0.0023, \tval-acc: 0.8984\n",
            "Epoch: 23 \t train-Loss: 0.0017, \tval-Loss: 0.0023, \tval-acc: 0.9038\n",
            "Epoch: 24 \t train-Loss: 0.0016, \tval-Loss: 0.0023, \tval-acc: 0.9020\n",
            "Epoch: 25 \t train-Loss: 0.0015, \tval-Loss: 0.0022, \tval-acc: 0.9087\n",
            "Epoch: 26 \t train-Loss: 0.0014, \tval-Loss: 0.0023, \tval-acc: 0.9046\n",
            "Epoch: 27 \t train-Loss: 0.0013, \tval-Loss: 0.0022, \tval-acc: 0.9033\n",
            "Epoch: 28 \t train-Loss: 0.0012, \tval-Loss: 0.0022, \tval-acc: 0.9050\n",
            "Epoch: 29 \t train-Loss: 0.0012, \tval-Loss: 0.0023, \tval-acc: 0.9040\n",
            "Epoch: 30 \t train-Loss: 0.0011, \tval-Loss: 0.0023, \tval-acc: 0.9035\n",
            "Epoch: 31 \t train-Loss: 0.0011, \tval-Loss: 0.0023, \tval-acc: 0.9045\n",
            "Epoch: 32 \t train-Loss: 0.0011, \tval-Loss: 0.0024, \tval-acc: 0.8993\n",
            "Epoch: 33 \t train-Loss: 0.0010, \tval-Loss: 0.0023, \tval-acc: 0.9051\n",
            "Epoch: 34 \t train-Loss: 0.0010, \tval-Loss: 0.0024, \tval-acc: 0.9053\n",
            "Epoch: 35 \t train-Loss: 0.0010, \tval-Loss: 0.0024, \tval-acc: 0.8980\n",
            "Epoch: 36 \t train-Loss: 0.0009, \tval-Loss: 0.0023, \tval-acc: 0.9062\n",
            "Epoch: 37 \t train-Loss: 0.0009, \tval-Loss: 0.0025, \tval-acc: 0.9034\n",
            "Epoch: 38 \t train-Loss: 0.0009, \tval-Loss: 0.0025, \tval-acc: 0.8997\n",
            "Epoch: 39 \t train-Loss: 0.0009, \tval-Loss: 0.0025, \tval-acc: 0.8995\n",
            "Epoch: 40 \t train-Loss: 0.0009, \tval-Loss: 0.0025, \tval-acc: 0.9023\n",
            "Epoch: 41 \t train-Loss: 0.0007, \tval-Loss: 0.0021, \tval-acc: 0.9169\n",
            "Epoch: 42 \t train-Loss: 0.0006, \tval-Loss: 0.0021, \tval-acc: 0.9167\n",
            "Epoch: 43 \t train-Loss: 0.0005, \tval-Loss: 0.0021, \tval-acc: 0.9178\n",
            "Epoch: 44 \t train-Loss: 0.0005, \tval-Loss: 0.0021, \tval-acc: 0.9174\n",
            "Epoch: 45 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9176\n",
            "Epoch: 46 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9181\n",
            "Epoch: 47 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9185\n",
            "Epoch: 48 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9187\n",
            "Epoch: 49 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9191\n",
            "Epoch: 50 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9180\n",
            "Epoch: 51 \t train-Loss: 0.0004, \tval-Loss: 0.0021, \tval-acc: 0.9200\n",
            "Epoch: 52 \t train-Loss: 0.0003, \tval-Loss: 0.0021, \tval-acc: 0.9182\n",
            "Epoch: 53 \t train-Loss: 0.0003, \tval-Loss: 0.0021, \tval-acc: 0.9194\n",
            "Epoch: 54 \t train-Loss: 0.0003, \tval-Loss: 0.0021, \tval-acc: 0.9197\n",
            "Epoch: 55 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9194\n",
            "Epoch: 56 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9187\n",
            "Epoch: 57 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9201\n",
            "Epoch: 58 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9207\n",
            "Epoch: 59 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9190\n",
            "Epoch: 60 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9202\n",
            "Epoch: 61 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9202\n",
            "Epoch: 62 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9203\n",
            "Epoch: 63 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9194\n",
            "Epoch: 64 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9206\n",
            "Epoch: 65 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9192\n",
            "Epoch: 66 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9203\n",
            "Epoch: 67 \t train-Loss: 0.0002, \tval-Loss: 0.0022, \tval-acc: 0.9206\n",
            "Epoch: 68 \t train-Loss: 0.0002, \tval-Loss: 0.0022, \tval-acc: 0.9198\n",
            "Epoch: 69 \t train-Loss: 0.0002, \tval-Loss: 0.0022, \tval-acc: 0.9197\n",
            "Epoch: 70 \t train-Loss: 0.0003, \tval-Loss: 0.0022, \tval-acc: 0.9201\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cuP1cPQCelR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}