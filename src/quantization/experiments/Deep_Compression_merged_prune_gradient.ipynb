{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Deep_Compression_merged_prune_gradient (1).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlVSCmvnf7YH"
      },
      "source": [
        "# imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqQKwO50f1kU",
        "outputId": "f4219809-b5f8-4757-cc6c-bccf514d09ca"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "checkpoint_loc = '/content/gdrive/MyDrive/11785/project/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FG2hQJH6gIzB"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchvision.datasets import MNIST, CIFAR10, CIFAR100\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import os\n",
        "import torch, torchvision\n",
        "from torchvision import transforms\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "import types"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoA2K8xugTBX"
      },
      "source": [
        "torch.manual_seed(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "INIT_LR = 0.1\n",
        "WEIGHT_DECAY_RATE = 0.0005\n",
        "EPOCHS = 70\n",
        "lr_decay_interval = 10\n",
        "batch_size = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SGsK22PdgUbJ"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "311dN2x2Kvyq",
        "outputId": "89a4867b-1f80-4e6d-fd48-7bf77e4a649a"
      },
      "source": [
        "'''class BasicBlock(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None, base_width=1, padding=1, batch_norm=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        if batch_norm is None:\n",
        "            bn_layer = nn.Batchnorm2D\n",
        "        else:\n",
        "            bn_layer = batch_norm\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding, bias=False)\n",
        "        self.bn1 = bn_layer(out_channels)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=1, stride=1, bias=False)\n",
        "        self.bn2 = bn_layer(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "        if self.downsample is None:\n",
        "            self.shortcut = nn.Identity()\n",
        "        else:\n",
        "            self.shortcut = self.downsample\n",
        "\n",
        "        self.stride = stride\n",
        "\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu1(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        out += self.shortcut(x)\n",
        "        out = self.relu2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet34(nn.Module):\n",
        "\n",
        "    def __init__(self, layers, num_classes=100, zero_init_residual=False, base_width=64, batch_norm=None):\n",
        "        # def make_layer(self, block, planes, blocks, stride= 1, dilate = False):\n",
        "        super(ResNet34, self).__init__()\n",
        "        block = BasicBlock\n",
        "        if batch_norm is None:\n",
        "            bn_layer = nn.BatchNorm2d\n",
        "        self.bn_layer = bn_layer\n",
        "\n",
        "        self.conv_out_channels = 64\n",
        "        self.in_channels = self.conv_out_channels\n",
        "        self.base_width = base_width\n",
        "        self.conv1 = nn.Conv2d(3, self.conv_out_channels, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = bn_layer(self.conv_out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        self.layer1 = self.container(block, 64, layers[0])\n",
        "        self.layer2 = self.container(block, 128, layers[1], stride=2, dilate=False)\n",
        "        self.layer3 = self.container(block, 256, layers[2], stride=2, dilate=False)\n",
        "        self.layer4 = self.container(block, 512, layers[3], stride=2, dilate=False)\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def container(self, block, in_channels, num_basicblocks, stride=1, dilate=False):\n",
        "        bn_layer = self.bn_layer\n",
        "        downsample = None\n",
        "        if stride != 1:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.in_channels, in_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                bn_layer(in_channels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(\n",
        "            block(self.in_channels, in_channels, stride, downsample, self.base_width, padding=1, batch_norm=bn_layer))\n",
        "        self.in_channels = in_channels\n",
        "        for basic_blocks in range(1, num_basicblocks):\n",
        "            layers.append(block(self.in_channels, in_channels, base_width=self.base_width, batch_norm=bn_layer))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    num_epochs = 30\n",
        "\n",
        "    num_classes = 100\n",
        "\n",
        "    net = ResNet34([3, 4, 6, 3], num_classes=100)\n",
        "    network = net.to(device)\n",
        "    learningRate = 0.1\n",
        "    weightDecay = 5e-5\n",
        "    criterion_label = nn.CrossEntropyLoss()\n",
        "    optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "    network.train()\n",
        "    network.to(device)\n",
        "    print(network)'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "ResNet34(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (3): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (4): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (5): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (2): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu1): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Identity()\n",
            "      (relu2): ReLU(inplace=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=100, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kg2Bb1RMDqaq",
        "outputId": "ad16928e-8729-41df-f81f-5770acb1343e"
      },
      "source": [
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=100):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def ResNet50():\n",
        "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR100(root='./data', train=True,\n",
        "                                            download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=32,\n",
        "                                              shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR100(root='./data', train=False,\n",
        "                                           download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=32,\n",
        "                                             shuffle=False, num_workers=2)\n",
        "\n",
        "    num_epochs = 30\n",
        "\n",
        "    num_classes = 100\n",
        "\n",
        "    net = ResNet50()\n",
        "    network = net.to(device)\n",
        "    learningRate = 0.1\n",
        "    weightDecay = 5e-5\n",
        "    criterion_label = nn.CrossEntropyLoss()\n",
        "    optimizer_label = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "    network.train()\n",
        "    network.to(device)\n",
        "    print(network)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78H6ZQVrgb9C"
      },
      "source": [
        "# Dataset and loaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8P0SQQeNgg30"
      },
      "source": [
        "def get_cifar100_dataloaders(train_batch_size, test_batch_size):\n",
        "\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465),\n",
        "                             (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    train_dataset = CIFAR100('_dataset', True, train_transform, download=True)\n",
        "    test_dataset = CIFAR100('_dataset', False, test_transform, download=False)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        train_batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        test_batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boYCdWBOgjJa"
      },
      "source": [
        "def cifar100_experiment():\n",
        "    \n",
        "    BATCH_SIZE = 128\n",
        "    LR_DECAY_INTERVAL = 20\n",
        "    \n",
        "    #net = VGG_SNIP('D').to(device)\n",
        "    # net = \n",
        "    #optimiser = optim.SGD(\n",
        "    #    net.parameters(),\n",
        "    #    lr=INIT_LR,\n",
        "    #    momentum=0.9,\n",
        "    #    weight_decay=WEIGHT_DECAY_RATE)\n",
        "    #lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "    #    optimiser, LR_DECAY_INTERVAL, gamma=0.1)\n",
        "    #net = ResNet34([3, 4, 6, 3], num_classes=100)\n",
        "    net = ResNet50()\n",
        "    network = net.to(device)\n",
        "    learningRate = 0.1\n",
        "    weightDecay = 5e-5\n",
        "    criterion_label = nn.CrossEntropyLoss()\n",
        "    optimiser = torch.optim.SGD(network.parameters(), lr=learningRate, weight_decay=weightDecay, momentum=0.9)\n",
        "    lr_scheduler = optim.lr_scheduler.StepLR(\n",
        "        optimiser, LR_DECAY_INTERVAL, gamma=0.1)\n",
        "    network.train()\n",
        "    network.to(device)\n",
        "    print(network)\n",
        "    \n",
        "    train_loader, val_loader = get_cifar100_dataloaders(BATCH_SIZE,\n",
        "                                                       BATCH_SIZE)  # TODO\n",
        "\n",
        "    return net, optimiser, lr_scheduler, train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRYSlTNrgkJs",
        "outputId": "07d7a000-cf81-46d4-f4a4-b107b2d8c741"
      },
      "source": [
        "initial_net, optimiser, lr_scheduler, train_loader, val_loader = cifar100_experiment()\n",
        "initial_net = initial_net.to(device)\n",
        "torch.save(initial_net,'/content/init.pt')\n",
        "initial_net"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n",
            ")\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (shortcut): Sequential()\n",
              "    )\n",
              "  )\n",
              "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jNCk-s9i6G8"
      },
      "source": [
        "# Pruning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFIi-BL8vZMk"
      },
      "source": [
        "SNIP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c8pwhf7sbdy"
      },
      "source": [
        "def pruning(model, density):\n",
        "\n",
        "    grad_list, mask, weight_list = [], [], []\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            grad_list += list(abs(m.weight.grad.flatten().cpu().detach().numpy()))\n",
        "            weight_list += list(abs(m.weight.flatten().cpu().detach().numpy()))\n",
        "\n",
        "    threshold_grad = np.percentile(np.array((grad_list)), 100-density)\n",
        "    threshold_weight = np.percentile(np.array((weight_list)), 100-density)\n",
        "\n",
        "    weight_sparsity_check = np.where((weight_list)>=threshold_weight, 1, 0).sum()/len(weight_list)\n",
        "    grad_sparsity_check = np.where((grad_list)>=threshold_grad, 1, 0).sum()/len(grad_list)\n",
        "\n",
        "    # print(weight_sparsity_check, grad_sparsity_check)\n",
        "\n",
        "    sums = 0\n",
        "    for m in model.modules():\n",
        "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
        "            gradmask_numpy = np.where(abs(m.weight.grad.cpu().detach().numpy())>=threshold_grad, 1, 0)\n",
        "            weightmask_numpy = np.where(abs(m.weight.cpu().detach().numpy())>=threshold_weight, 1, 0)\n",
        "            weight_grad = np.logical_or(gradmask_numpy, weightmask_numpy).astype(float)\n",
        "            sums += weight_grad.sum()\n",
        "            # mask.append(torch.from_numpy(gradmask_numpy).cuda())\n",
        "            mask.append(torch.from_numpy(weight_grad).cuda())\n",
        "            # print(mask_numpy.shape)\n",
        "        \n",
        "    # print(len(mask))\n",
        "    # print(sums/len(weight_list))\n",
        "    del grad_list\n",
        "    del weight_list\n",
        "    del weightmask_numpy\n",
        "    del gradmask_numpy\n",
        "    del weight_grad\n",
        "    \n",
        "    return mask\n",
        "\n",
        "def apply_prune_mask(model, keep_masks):\n",
        "\n",
        "    prunable_layers = filter(\n",
        "        lambda layer: isinstance(layer, nn.Conv2d) or isinstance(\n",
        "            layer, nn.Linear), model.modules())\n",
        "\n",
        "    for layer, keep_mask in zip(prunable_layers, keep_masks):\n",
        "        assert (layer.weight.shape == keep_mask.shape)\n",
        "\n",
        "        layer.weight.data[keep_mask == 0.] = 0.\n",
        "        # print(100*np.count_nonzero(layer.weight.clone().cpu().detach().numpy())/(layer.weight.clone().flatten()).shape[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF-FcEgVjbnY"
      },
      "source": [
        "# Pruning while training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTQd0fYj38y"
      },
      "source": [
        "def training(epoch, model, optimizer, scheduler, criterion, device, train_loader):\n",
        "  model.train()\n",
        "  avg_loss = 0.0\n",
        "  av_loss=0.0\n",
        "  total=0\n",
        "  for batch_num, (feats, labels) in enumerate(train_loader):\n",
        "      feats, labels = feats.to(device), labels.to(device)\n",
        "      \n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = model(feats)\n",
        "\n",
        "\n",
        "      loss = criterion(outputs, labels.long())\n",
        "      loss.backward()\n",
        "      \n",
        "      optimizer.step()\n",
        "      \n",
        "      avg_loss += loss.item()\n",
        "      av_loss += loss.item() \n",
        "      total +=len(feats) \n",
        "      # if batch_num % 10 == 9:\n",
        "      #     print('Epoch: {}\\tBatch: {}\\tAv-Loss: {:.4f}'.format(epoch+1, batch_num+1, av_loss/10))\n",
        "      #     av_loss = 0.0\n",
        "\n",
        "      torch.cuda.empty_cache()\n",
        "      del feats\n",
        "      del labels\n",
        "      del loss\n",
        "\n",
        "  del train_loader\n",
        "  return avg_loss/total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhOefkNFwG3S"
      },
      "source": [
        "import time\n",
        "\n",
        "def validate(epoch, model, criterion, device, data_loader):\n",
        "    start_time = time.time()\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        running_loss, accuracy,total  = 0.0, 0.0, 0\n",
        "\n",
        "        \n",
        "        for i, (X, Y) in enumerate(data_loader):\n",
        "            \n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            output= model(X)\n",
        "            loss = criterion(output, Y.long())\n",
        "\n",
        "            _,pred_labels = torch.max(F.softmax(output, dim=1), 1)\n",
        "            pred_labels = pred_labels.view(-1)\n",
        "            \n",
        "            accuracy += torch.sum(torch.eq(pred_labels, Y)).item()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            total += len(X)\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            \n",
        "            del X\n",
        "            del Y\n",
        "        \n",
        "        return running_loss/total, accuracy/total, (time.time() - start_time)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "DJrAtHiArxhL",
        "outputId": "833a49da-790e-4ba5-a447-2392f8f714e3"
      },
      "source": [
        "after_pruning_net, optimiser, lr_scheduler, train_loader, val_loader = cifar100_experiment()\n",
        "after_pruning_net = after_pruning_net.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "density = 5\n",
        "\n",
        "train_loss = training(1, after_pruning_net, optimiser, lr_scheduler, criterion, device,train_loader)\n",
        "val_loss, val_acc,_ = validate(1, after_pruning_net, criterion, device, val_loader)\n",
        "# Pre-training pruning using SKIP\n",
        "\n",
        "keep_masks = pruning(after_pruning_net, density)  \n",
        "apply_prune_mask(after_pruning_net, keep_masks)\n",
        "max = 0\n",
        "path = 'after_pruning.ptmodel'\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    if os.path.exists(path):\n",
        "      checkpoint = torch.load(path)\n",
        "      #after_pruning_net.load_state_dict(checkpoint['state_dict'])\n",
        "    train_loss = training(epoch, after_pruning_net, optimiser, lr_scheduler, criterion, device,train_loader)\n",
        "\n",
        "    val_loss, val_acc,_ = validate(epoch, after_pruning_net, criterion, device, val_loader)\n",
        "\n",
        "    if max < val_acc*100:\n",
        "      torch.save({'state_dict': after_pruning_net.state_dict()}, path)\n",
        "    lr_scheduler.step()\n",
        "    apply_prune_mask(after_pruning_net, keep_masks)\n",
        "\n",
        "    print('Epoch: {} \\t train-Loss: {:.4f}, \\tval-Loss: {:.4f}, \\tval-acc: {:.4f}'.format(epoch+1,  train_loss, val_loss, val_acc))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (layer1): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (3): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (4): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (5): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): Bottleneck(\n",
            "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential(\n",
            "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "    (2): Bottleneck(\n",
            "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (shortcut): Sequential()\n",
            "    )\n",
            "  )\n",
            "  (linear): Linear(in_features=2048, out_features=100, bias=True)\n",
            ")\n",
            "Files already downloaded and verified\n",
            "Epoch: 1 \t train-Loss: 0.0355, \tval-Loss: 0.0319, \tval-acc: 0.0702\n",
            "Epoch: 2 \t train-Loss: 0.0324, \tval-Loss: 0.0304, \tval-acc: 0.1047\n",
            "Epoch: 3 \t train-Loss: 0.0305, \tval-Loss: 0.0312, \tval-acc: 0.1248\n",
            "Epoch: 4 \t train-Loss: 0.0295, \tval-Loss: 0.0283, \tval-acc: 0.1458\n",
            "Epoch: 5 \t train-Loss: 0.0278, \tval-Loss: 0.0267, \tval-acc: 0.1868\n",
            "Epoch: 6 \t train-Loss: 0.0265, \tval-Loss: 0.0252, \tval-acc: 0.2224\n",
            "Epoch: 7 \t train-Loss: 0.0253, \tval-Loss: 0.0239, \tval-acc: 0.2524\n",
            "Epoch: 8 \t train-Loss: 0.0241, \tval-Loss: 0.0228, \tval-acc: 0.2738\n",
            "Epoch: 9 \t train-Loss: 0.0228, \tval-Loss: 0.0223, \tval-acc: 0.2981\n",
            "Epoch: 10 \t train-Loss: 0.0217, \tval-Loss: 0.0217, \tval-acc: 0.3121\n",
            "Epoch: 11 \t train-Loss: 0.0206, \tval-Loss: 0.0202, \tval-acc: 0.3439\n",
            "Epoch: 12 \t train-Loss: 0.0195, \tval-Loss: 0.0191, \tval-acc: 0.3729\n",
            "Epoch: 13 \t train-Loss: 0.0184, \tval-Loss: 0.0182, \tval-acc: 0.3951\n",
            "Epoch: 14 \t train-Loss: 0.0175, \tval-Loss: 0.0171, \tval-acc: 0.4245\n",
            "Epoch: 15 \t train-Loss: 0.0168, \tval-Loss: 0.0169, \tval-acc: 0.4371\n",
            "Epoch: 16 \t train-Loss: 0.0161, \tval-Loss: 0.0173, \tval-acc: 0.4344\n",
            "Epoch: 17 \t train-Loss: 0.0155, \tval-Loss: 0.0166, \tval-acc: 0.4511\n",
            "Epoch: 18 \t train-Loss: 0.0150, \tval-Loss: 0.0160, \tval-acc: 0.4587\n",
            "Epoch: 19 \t train-Loss: 0.0144, \tval-Loss: 0.0154, \tval-acc: 0.4854\n",
            "Epoch: 20 \t train-Loss: 0.0141, \tval-Loss: 0.0151, \tval-acc: 0.4931\n",
            "Epoch: 21 \t train-Loss: 0.0115, \tval-Loss: 0.0124, \tval-acc: 0.5701\n",
            "Epoch: 22 \t train-Loss: 0.0112, \tval-Loss: 0.0124, \tval-acc: 0.5715\n",
            "Epoch: 23 \t train-Loss: 0.0110, \tval-Loss: 0.0122, \tval-acc: 0.5767\n",
            "Epoch: 24 \t train-Loss: 0.0108, \tval-Loss: 0.0123, \tval-acc: 0.5756\n",
            "Epoch: 25 \t train-Loss: 0.0107, \tval-Loss: 0.0121, \tval-acc: 0.5832\n",
            "Epoch: 26 \t train-Loss: 0.0106, \tval-Loss: 0.0121, \tval-acc: 0.5791\n",
            "Epoch: 27 \t train-Loss: 0.0104, \tval-Loss: 0.0119, \tval-acc: 0.5863\n",
            "Epoch: 28 \t train-Loss: 0.0103, \tval-Loss: 0.0119, \tval-acc: 0.5839\n",
            "Epoch: 29 \t train-Loss: 0.0102, \tval-Loss: 0.0120, \tval-acc: 0.5852\n",
            "Epoch: 30 \t train-Loss: 0.0101, \tval-Loss: 0.0118, \tval-acc: 0.5898\n",
            "Epoch: 31 \t train-Loss: 0.0100, \tval-Loss: 0.0119, \tval-acc: 0.5886\n",
            "Epoch: 32 \t train-Loss: 0.0099, \tval-Loss: 0.0119, \tval-acc: 0.5871\n",
            "Epoch: 33 \t train-Loss: 0.0098, \tval-Loss: 0.0118, \tval-acc: 0.5883\n",
            "Epoch: 34 \t train-Loss: 0.0097, \tval-Loss: 0.0121, \tval-acc: 0.5855\n",
            "Epoch: 35 \t train-Loss: 0.0096, \tval-Loss: 0.0117, \tval-acc: 0.5895\n",
            "Epoch: 36 \t train-Loss: 0.0095, \tval-Loss: 0.0116, \tval-acc: 0.5952\n",
            "Epoch: 37 \t train-Loss: 0.0094, \tval-Loss: 0.0117, \tval-acc: 0.5951\n",
            "Epoch: 38 \t train-Loss: 0.0093, \tval-Loss: 0.0115, \tval-acc: 0.5990\n",
            "Epoch: 39 \t train-Loss: 0.0092, \tval-Loss: 0.0116, \tval-acc: 0.6006\n",
            "Epoch: 40 \t train-Loss: 0.0091, \tval-Loss: 0.0115, \tval-acc: 0.5986\n",
            "Epoch: 41 \t train-Loss: 0.0084, \tval-Loss: 0.0111, \tval-acc: 0.6108\n",
            "Epoch: 42 \t train-Loss: 0.0084, \tval-Loss: 0.0111, \tval-acc: 0.6110\n",
            "Epoch: 43 \t train-Loss: 0.0084, \tval-Loss: 0.0111, \tval-acc: 0.6136\n",
            "Epoch: 44 \t train-Loss: 0.0084, \tval-Loss: 0.0111, \tval-acc: 0.6124\n",
            "Epoch: 45 \t train-Loss: 0.0083, \tval-Loss: 0.0111, \tval-acc: 0.6141\n",
            "Epoch: 46 \t train-Loss: 0.0083, \tval-Loss: 0.0111, \tval-acc: 0.6148\n",
            "Epoch: 47 \t train-Loss: 0.0082, \tval-Loss: 0.0111, \tval-acc: 0.6138\n",
            "Epoch: 48 \t train-Loss: 0.0083, \tval-Loss: 0.0111, \tval-acc: 0.6158\n",
            "Epoch: 49 \t train-Loss: 0.0082, \tval-Loss: 0.0111, \tval-acc: 0.6162\n",
            "Epoch: 50 \t train-Loss: 0.0082, \tval-Loss: 0.0111, \tval-acc: 0.6166\n",
            "Epoch: 51 \t train-Loss: 0.0082, \tval-Loss: 0.0111, \tval-acc: 0.6165\n",
            "Epoch: 52 \t train-Loss: 0.0082, \tval-Loss: 0.0111, \tval-acc: 0.6153\n",
            "Epoch: 53 \t train-Loss: 0.0081, \tval-Loss: 0.0111, \tval-acc: 0.6163\n",
            "Epoch: 54 \t train-Loss: 0.0081, \tval-Loss: 0.0110, \tval-acc: 0.6157\n",
            "Epoch: 55 \t train-Loss: 0.0081, \tval-Loss: 0.0111, \tval-acc: 0.6174\n",
            "Epoch: 56 \t train-Loss: 0.0081, \tval-Loss: 0.0110, \tval-acc: 0.6160\n",
            "Epoch: 57 \t train-Loss: 0.0081, \tval-Loss: 0.0111, \tval-acc: 0.6183\n",
            "Epoch: 58 \t train-Loss: 0.0081, \tval-Loss: 0.0111, \tval-acc: 0.6168\n",
            "Epoch: 59 \t train-Loss: 0.0081, \tval-Loss: 0.0111, \tval-acc: 0.6175\n",
            "Epoch: 60 \t train-Loss: 0.0080, \tval-Loss: 0.0111, \tval-acc: 0.6176\n",
            "Epoch: 61 \t train-Loss: 0.0079, \tval-Loss: 0.0111, \tval-acc: 0.6172\n",
            "Epoch: 62 \t train-Loss: 0.0079, \tval-Loss: 0.0111, \tval-acc: 0.6197\n",
            "Epoch: 63 \t train-Loss: 0.0079, \tval-Loss: 0.0110, \tval-acc: 0.6191\n",
            "Epoch: 64 \t train-Loss: 0.0079, \tval-Loss: 0.0111, \tval-acc: 0.6166\n",
            "Epoch: 65 \t train-Loss: 0.0079, \tval-Loss: 0.0111, \tval-acc: 0.6190\n",
            "Epoch: 66 \t train-Loss: 0.0079, \tval-Loss: 0.0110, \tval-acc: 0.6178\n",
            "Epoch: 67 \t train-Loss: 0.0079, \tval-Loss: 0.0110, \tval-acc: 0.6187\n",
            "Epoch: 68 \t train-Loss: 0.0079, \tval-Loss: 0.0111, \tval-acc: 0.6183\n",
            "Epoch: 69 \t train-Loss: 0.0079, \tval-Loss: 0.0110, \tval-acc: 0.6186\n",
            "Epoch: 70 \t train-Loss: 0.0079, \tval-Loss: 0.0111, \tval-acc: 0.6189\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NB9l6iFR7FzR"
      },
      "source": [
        "# torch.save(net,'/content/gdrive/MyDrive/11785/project/after_pruning.ptmodel')\n",
        "#after_pruning_net = torch.load('/content/gdrive/MyDrive/11785/project/after_pruning.ptmodel')\n",
        "# net = torch.load('after_pruning.ptmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S3bwXnTpJ6v",
        "outputId": "6e71edc0-d8b2-428d-d730-064f4546c1c1"
      },
      "source": [
        " criterion = nn.CrossEntropyLoss()\n",
        " val_loss, val_acc,time_taken = validate(0, after_pruning_net, criterion, device, val_loader)\n",
        " print(val_loss, ' ', val_acc,' ',time_taken)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.016427882385253906   0.4841   1.4642391204833984\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOb17YiRr0Dy"
      },
      "source": [
        "# Quantization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAwR8XXd6Ik-"
      },
      "source": [
        "Sparse matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        },
        "id": "zK3rMqHB9x3f",
        "outputId": "4bcc823b-97e0-477b-f800-7d2bd68c52e7"
      },
      "source": [
        "import copy\n",
        "quantization_net = copy.deepcopy(after_pruning_net)\n",
        "quantization_net.features\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-43cbcd653040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mquantization_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter_pruning_net\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mquantization_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    946\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 948\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'ResNet34' object has no attribute 'features'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9rcx6D1dMch"
      },
      "source": [
        "Feature wide quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lIE0Qk66crJ",
        "outputId": "58c86c69-e857-400d-9274-b943c905ff78"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "bits = 5\n",
        "for layer, (name, module) in enumerate(quantization_net.features._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    dev = module.weight.device\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    org_shape =  module.weight.shape\n",
        "\n",
        "    flatten_weights = weight.flatten()\n",
        "    min_ = np.min(flatten_weights)\n",
        "    max_ = np.max(flatten_weights)\n",
        "    space = np.linspace(min_, max_, num=2**bits)\n",
        "\n",
        "    print(module.weight.flatten().size())\n",
        "    kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
        "    kmeans.fit(weight.reshape(-1,1))\n",
        "    new_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(-1)\n",
        "    mat = new_weight.reshape(org_shape)\n",
        "    module.weight.data = torch.from_numpy(mat).to(dev)\n",
        "\n",
        "  else:\n",
        "    print('skipped')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------  name: Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([1728])\n",
            "----------  name: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "torch.Size([64])\n",
            "----------  name: ReLU(inplace=True)\n",
            "skipped\n",
            "----------  name: Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([36864])\n",
            "----------  name: BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "torch.Size([64])\n",
            "----------  name: ReLU(inplace=True)\n",
            "skipped\n",
            "----------  name: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "skipped\n",
            "----------  name: Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([73728])\n",
            "----------  name: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "torch.Size([128])\n",
            "----------  name: ReLU(inplace=True)\n",
            "skipped\n",
            "----------  name: Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([147456])\n",
            "----------  name: BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "torch.Size([128])\n",
            "----------  name: ReLU(inplace=True)\n",
            "skipped\n",
            "----------  name: MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "skipped\n",
            "----------  name: Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([294912])\n",
            "----------  name: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "torch.Size([256])\n",
            "----------  name: ReLU(inplace=True)\n",
            "skipped\n",
            "----------  name: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([589824])\n",
            "----------  name: BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "torch.Size([256])\n",
            "----------  name: ReLU(inplace=True)\n",
            "skipped\n",
            "----------  name: Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "torch.Size([589824])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOppJhK-dR7w"
      },
      "source": [
        "Classifier quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rifao3XedRas"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "import numpy as np\n",
        "bits = 5\n",
        "for layer, (name, module) in enumerate(quantization_net.classifier._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    dev = module.weight.device\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    org_shape =  module.weight.shape\n",
        "\n",
        "    flatten_weights = weight.flatten()\n",
        "    min_ = np.min(flatten_weights)\n",
        "    max_ = np.max(flatten_weights)\n",
        "    space = np.linspace(min_, max_, num=2**bits)\n",
        "\n",
        "    print(module.weight.flatten().size())\n",
        "    kmeans = KMeans(n_clusters=len(space), init=space.reshape(-1,1), n_init=1, precompute_distances=True, algorithm=\"full\")\n",
        "    kmeans.fit(weight.reshape(-1,1))\n",
        "    new_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(-1)\n",
        "    mat = new_weight.reshape(org_shape)\n",
        "    module.weight.data = torch.from_numpy(mat).to(dev)\n",
        "\n",
        "  else:\n",
        "    print('skipped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7v5Z0VIn35f"
      },
      "source": [
        "quantization_net = torch.load('/content/gdrive/MyDrive/11785/project/after_quantization.ptmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQ8Qx_gQUN_g"
      },
      "source": [
        " quantization_net.cuda()\n",
        " val_loss, val_acc,time_taken = validate(0, quantization_net, criterion, device, val_loader)\n",
        " print(val_acc,' ',time_taken)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf6CsKV-a_5r"
      },
      "source": [
        "torch.save(quantization_net,'after_quantization.ptmodel')\n",
        "torch.save(quantization_net,'/content/gdrive/MyDrive/11785/project/after_quantization.ptmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KoYVGtRgAjY"
      },
      "source": [
        "# Post quantization comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1l8LUpKcgHfi"
      },
      "source": [
        "layer_names = []\n",
        "post_quant_weights_unique = []\n",
        "for layer, (name, module) in enumerate(quantization_net.features._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    flatten_weights = weight.flatten()\n",
        "    layer_names.append(module.__class__.__name__)\n",
        "    post_quant_weights_unique.append(np.unique(flatten_weights))\n",
        "  else:\n",
        "    print('skipped')\n",
        "for layer, (name, module) in enumerate(quantization_net.classifier._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    flatten_weights = weight.flatten()\n",
        "    layer_names.append(module.__class__.__name__)\n",
        "    post_quant_weights_unique.append(np.unique(flatten_weights))\n",
        "  else:\n",
        "    print('skipped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evavO9Yyhk_W"
      },
      "source": [
        "layer_names = []\n",
        "pre_quant_weights_unique = []\n",
        "for layer, (name, module) in enumerate(after_pruning_net.features._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    flatten_weights = weight.flatten()\n",
        "    layer_names.append(module.__class__.__name__)\n",
        "    pre_quant_weights_unique.append(np.unique(flatten_weights))\n",
        "  else:\n",
        "    print('skipped')\n",
        "for layer, (name, module) in enumerate(after_pruning_net.classifier._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    flatten_weights = weight.flatten()\n",
        "    layer_names.append(module.__class__.__name__)\n",
        "    pre_quant_weights_unique.append(np.unique(flatten_weights))\n",
        "  else:\n",
        "    print('skipped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTdU5A6Bhqdj"
      },
      "source": [
        "if len(post_quant_weights_unique) == len(pre_quant_weights_unique):\n",
        "  print('true')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MshoAHLlh-Yf"
      },
      "source": [
        "i = 0\n",
        "for old_w,new_w in zip(pre_quant_weights_unique,post_quant_weights_unique):\n",
        "  print('layer_names [',layer_names[i],'] -> unique weights count old:',len(old_w),', new:',len(new_w))\n",
        "  i+=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_Mb2lj_i13p"
      },
      "source": [
        "import copy\n",
        "post_process_net = torch.load('/content/gdrive/MyDrive/11785/project/after_quantization.ptmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oqiPR3hpkfCB"
      },
      "source": [
        " val_loss, val_acc,time_taken = validate(0, post_process_net, criterion, device, val_loader)\n",
        " print(val_acc,time_taken)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bp2Vxo95sTGZ"
      },
      "source": [
        "import torch.quantization\n",
        "post_process_net = post_process_net.to('cpu')\n",
        "device='cpu'\n",
        "post_process_net.qconfig = torch.quantization.default_qconfig\n",
        "torch.quantization.prepare(post_process_net, inplace=True)\n",
        "torch.quantization.convert(post_process_net, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oX5fFT8VtblS"
      },
      "source": [
        " post_process_net.cpu()\n",
        " val_loss, val_acc,time_taken = validate(0, post_process_net, criterion, device, val_loader)\n",
        " print(val_acc,time_taken)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCnrWzHUkoDa"
      },
      "source": [
        "torch.save(net,'after_quantization_int.ptmodel')\n",
        "torch.save(net,'/content/gdrive/MyDrive/11785/project/after_quantization_int.ptmodel')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckHlXdItkzB-"
      },
      "source": [
        "layer_names = []\n",
        "post_quant_weights_int_unique = []\n",
        "for layer, (name, module) in enumerate(net.features._modules.items()):\n",
        "  print('-'*10,' name:', module)\n",
        "  if not isinstance(module,nn.ReLU) and not isinstance(module,nn.MaxPool2d):\n",
        "    weight = module.weight.data.cpu().numpy()\n",
        "    flatten_weights = weight.flatten()\n",
        "    post_quant_weights_int_unique.append(np.unique(flatten_weights))\n",
        "  else:\n",
        "    print('skipped')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPbyqL1EkVpw"
      },
      "source": [
        "# Visualization convolution weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUGx-cxCkepO"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plot_filters_multi_channel(t,img_name):\n",
        "    \n",
        "    #get the number of kernals\n",
        "    num_kernels = t.shape[0]    \n",
        "    \n",
        "    #define number of columns for subplots\n",
        "    num_cols = 12\n",
        "    #rows = num of kernels\n",
        "    num_rows = num_kernels\n",
        "    \n",
        "    #set the figure size\n",
        "    fig = plt.figure(figsize=(num_cols,num_rows))\n",
        "    \n",
        "    #looping through all the kernels\n",
        "    for i in range(t.shape[0]):\n",
        "        ax1 = fig.add_subplot(num_rows,num_cols,i+1)\n",
        "        \n",
        "        #for each kernel, we convert the tensor to numpy \n",
        "        npimg = np.array(t[i].numpy(), np.float32)\n",
        "        #standardize the numpy image\n",
        "        npimg = (npimg - np.mean(npimg)) / np.std(npimg)\n",
        "        npimg = np.minimum(1, np.maximum(0, (npimg + 0.5)))\n",
        "        npimg = npimg.transpose((1, 2, 0))\n",
        "        ax1.imshow(npimg)\n",
        "        ax1.axis('off')\n",
        "        ax1.set_title(str(i))\n",
        "        ax1.set_xticklabels([])\n",
        "        ax1.set_yticklabels([])\n",
        "        \n",
        "    plt.savefig(img_name, dpi=100)    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49U4g30moi9m"
      },
      "source": [
        "def plot_weights(model, layer_num,img_name):\n",
        "  \n",
        "  layer = model.features[layer_num]\n",
        "  \n",
        "  if isinstance(layer, nn.Conv2d):\n",
        "    weight_tensor = model.features[layer_num].weight.data\n",
        "    if weight_tensor.shape[1] == 3:\n",
        "      plot_filters_multi_channel(weight_tensor,img_name)\n",
        "    else:\n",
        "        print(\"Can only plot weights with three channels with single channel = False\")     \n",
        "  else:\n",
        "    print(\"Can only visualize layers which are convolutional\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRLTBRGHozl1"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "initial_net = initial_net.to('cpu')\n",
        "after_pruning_net = after_pruning_net.to('cpu')\n",
        "quantization_net = quantization_net.to('cpu')\n",
        "plot_weights(initial_net, 0,'initial state of weights')\n",
        "plot_weights(after_pruning_net, 0,'after pruning')\n",
        "plot_weights(quantization_net, 0,'after quantization with k-means')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5pm7_XWQptW"
      },
      "source": [
        "# Visualization Activation layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "id": "mjsmyyZCQysW",
        "outputId": "e0d2e267-b975-4c76-a281-678bed00c318"
      },
      "source": [
        "print(quantization_net.classifier[0])\n",
        "quantization_net = quantization_net.to('cpu')\n",
        "print(np.unique(quantization_net.classifier[0].weight.data.flatten()))\n",
        "plt.imshow(quantization_net.classifier[0].weight.data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear(in_features=512, out_features=512, bias=True)\n",
            "[-2.62651205e-01 -2.00239673e-01 -1.52694285e-01 -1.30984098e-01\n",
            " -1.11580133e-01 -9.03302729e-02 -6.05782866e-02 -4.06768061e-02\n",
            " -2.74116416e-02 -1.84960216e-02 -1.17350705e-02 -5.85467555e-03\n",
            " -2.37078825e-03 -6.54769887e-04  2.78642801e-06  2.29578675e-03\n",
            "  6.95845997e-03  1.36291971e-02  2.15460546e-02  3.15798894e-02\n",
            "  4.34518382e-02  5.56850210e-02  6.92472532e-02  7.85576776e-02\n",
            "  9.07434896e-02  1.12685129e-01  1.39182061e-01  1.52690858e-01\n",
            "  1.75977081e-01  1.93883166e-01  2.74437904e-01  4.24591839e-01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f3108f54150>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}